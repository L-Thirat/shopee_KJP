{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shopee.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1ADeoIGPGeP",
        "outputId": "d4889871-2100-49a8-8316-f50dad11a931"
      },
      "source": [
        "!git clone https://github.com/L-Thirat/shopee_KJP.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'shopee_KJP'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 38 (delta 10), reused 36 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (38/38), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B7GjMvGPjRB",
        "outputId": "098e77e5-4382-4cf3-aceb-06f57ce394d7"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/  \u001b[01;34mshopee_KJP\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5290rGQ0Ptc_",
        "outputId": "612e9599-cb63-41f9-cc10-2180b00d18cf"
      },
      "source": [
        "cd shopee_KJP/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/shopee_KJP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk6Wj4UdPw53",
        "outputId": "e740fc02-227d-47ba-dad5-1cd08f709629"
      },
      "source": [
        "import spacy\r\n",
        "import random\r\n",
        "import time\r\n",
        "import warnings\r\n",
        "import json\r\n",
        "from spacy.util import minibatch, compounding, decaying\r\n",
        "from spacy.gold import GoldParse\r\n",
        "from spacy.scorer import Scorer\r\n",
        "# from spacy.training import Example\r\n",
        "\r\n",
        "\r\n",
        "# Settings for google Collab\r\n",
        "# spacy.require_gpu()\r\n",
        "# gpu = spacy.prefer_gpu()\r\n",
        "# print('GPU:', gpu)\r\n",
        "\r\n",
        "\r\n",
        "# Downloading models\r\n",
        "# spacy.cli.download(\"en_core_web_sm\")\r\n",
        "# spacy.cli.download(\"en_core_web_lg\")\r\n",
        "\r\n",
        "\r\n",
        "train_ner_filename = \"train_ner.json\"\r\n",
        "val_ner_filename = \"val_ner.json\"\r\n",
        "\r\n",
        "outlog_file = 'output_log.txt'\r\n",
        "output_file = 'test_output.txt'\r\n",
        "train_file = 'train_output.txt'\r\n",
        "outlog_txt = 'outputlog.txt'\r\n",
        "\r\n",
        "# TRAIN_DATA = [('what is the price of polo?', {'entities': [(21, 25, 'PrdName')]}), ('what is the price of ball?', {'entities': [(21, 25, 'PrdName')]}), ('what is the price of jegging?', {'entities': [(21, 28, 'PrdName')]}), ('what is the price of t-shirt?', {'entities': [(21, 28, 'PrdName')]}), ('what is the price of jeans?', {'entities': [(21, 26, 'PrdName')]}), ('what is the price of bat?', {'entities': [(21, 24, 'PrdName')]}), ('what is the price of shirt?', {'entities': [(21, 26, 'PrdName')]}), ('what is the price of bag?', {'entities': [(21, 24, 'PrdName')]}), ('what is the price of cup?', {'entities': [(21, 24, 'PrdName')]}), ('what is the price of jug?', {'entities': [(21, 24, 'PrdName')]}), ('what is the price of plate?', {'entities': [(21, 26, 'PrdName')]}), ('what is the price of glass?', {'entities': [(21, 26, 'PrdName')]}), ('what is the price of moniter?', {'entities': [(21, 28, 'PrdName')]}), ('what is the price of desktop?', {'entities': [(21, 28, 'PrdName')]}), ('what is the price of bottle?', {'entities': [(21, 27, 'PrdName')]}), ('what is the price of mouse?', {'entities': [(21, 26, 'PrdName')]}), ('what is the price of keyboad?', {'entities': [(21, 28, 'PrdName')]}), ('what is the price of chair?', {'entities': [(21, 26, 'PrdName')]}), ('what is the price of table?', {'entities': [(21, 26, 'PrdName')]}), ('what is the price of watch?', {'entities': [(21, 26, 'PrdName')]})]\r\n",
        "with open(train_ner_filename, \"r\", encoding='utf-8') as json_file:\r\n",
        "    TRAIN_DATA = json.load(json_file)\r\n",
        "\r\n",
        "\r\n",
        "# TEST_DATA =  [('what is the price of polo?', {'entities': [(21, 25, 'PrdName')]}), ('what is the price of ball?', {'entities': [(21, 25, 'PrdName')]}), ('what is the price of jegging?', {'entities': [(21, 28, 'PrdName')]}), ('what is the price of t-shirt?', {'entities': [(21, 28, 'PrdName')]}), ('what is the price of jeans?', {'entities': [(21, 26, 'PrdName')]}), ('what is the price of bat?', {'entities': [(21, 24, 'PrdName')]}), ('what is the price of shirt?', {'entities': [(21, 26, 'PrdName')]}), ('what is the price of bag?', {'entities': [(21, 24, 'PrdName')]}), ('what is the price of cup?', {'entities': [(21, 24, 'PrdName')]}), ('what is the price of jug?', {'entities': [(21, 24, 'PrdName')]}), ('what is the price of plate?', {'entities': [(21, 26, 'PrdName')]}), ('what is the price of glass?', {'entities': [(21, 26, 'PrdName')]}), ('what is the price of moniter?', {'entities': [(21, 28, 'PrdName')]}), ('what is the price of desktop?', {'entities': [(21, 28, 'PrdName')]}), ('what is the price of bottle?', {'entities': [(21, 27, 'PrdName')]}), ('what is the price of mouse?', {'entities': [(21, 26, 'PrdName')]}), ('what is the price of keyboad?', {'entities': [(21, 28, 'PrdName')]}), ('what is the price of chair?', {'entities': [(21, 26, 'PrdName')]}), ('what is the price of table?', {'entities': [(21, 26, 'PrdName')]}), ('what is the price of watch?', {'entities': [(21, 26, 'PrdName')]})]\r\n",
        "with open(val_ner_filename, \"r\", encoding='utf-8') as json_file:\r\n",
        "    TEST_DATA = json.load(json_file)\r\n",
        "\r\n",
        "random.seed(0)\r\n",
        "\r\n",
        "# Log files for logging the train and testing scores for references\r\n",
        "file = open(outlog_file, 'w')\r\n",
        "file.write(\"iteration_no\" + \",\" + \"losses\" + \"\\n\")\r\n",
        "\r\n",
        "file1 = open(output_file, 'w')\r\n",
        "file1.write(\"iteration_no\" + \",\" + \"ents_p\" + \",\" + \"ents_r\" + \",\" + \"ents_f\" + \",\" + \"ents_per_type\" + \"\\n\")\r\n",
        "\r\n",
        "file2 = open(train_file, 'w')\r\n",
        "file2.write(\"iteration_no\" + \",\" + \"ents_p\" + \",\" + \"ents_r\" + \",\" + \"ents_f\" + \",\" + \"ents_per_type\" + \"\\n\")\r\n",
        "\r\n",
        "model = None  # (\"en_core_web_sm\")   # Replace with model you want to train\r\n",
        "start_training_time = time.time()\r\n",
        "\r\n",
        "\r\n",
        "def train_spacy(data, iterations):\r\n",
        "    if model is not None:\r\n",
        "        nlp = spacy.load(model)  # load existing spaCy model\r\n",
        "        print(\"Loaded model '%s'\" % model)\r\n",
        "    else:\r\n",
        "        nlp = spacy.blank(\"id\")  # create blank Language class\r\n",
        "        print(\"Created blank 'indo' model\")\r\n",
        "\r\n",
        "    TRAIN_DATA = data\r\n",
        "\r\n",
        "    # create the built-in pipeline components and add them to the pipeline\r\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\r\n",
        "    if 'ner' not in nlp.pipe_names:\r\n",
        "        ner = nlp.create_pipe('ner')\r\n",
        "        nlp.add_pipe(ner, last=True)\r\n",
        "        # ner = nlp.add_pipe(\"ner\")\r\n",
        "\r\n",
        "    else:\r\n",
        "        ner = nlp.get_pipe(\"ner\")\r\n",
        "\r\n",
        "    # add labels\r\n",
        "    for _, annotations in TRAIN_DATA:\r\n",
        "        for ent in annotations.get('entities'):\r\n",
        "            ner.add_label(ent[2])\r\n",
        "\r\n",
        "    if model is None:\r\n",
        "        optimizer = nlp.begin_training()\r\n",
        "\r\n",
        "        # For training with customized cfg\r\n",
        "        nlp.entity.cfg['conv_depth'] = 16\r\n",
        "        nlp.entity.cfg['token_vector_width'] = 256\r\n",
        "        # nlp.entity.cfg['bilstm_depth'] = 1\r\n",
        "        # nlp.entity.cfg['beam_width'] = 2\r\n",
        "\r\n",
        "\r\n",
        "    else:\r\n",
        "        print(\"resuming\")\r\n",
        "        optimizer = nlp.resume_training()\r\n",
        "        print(optimizer.learn_rate)\r\n",
        "\r\n",
        "    # get names of other pipes to disable them during training\r\n",
        "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\r\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\r\n",
        "\r\n",
        "    dropout = decaying(0.8, 0.2, 1e-6)  # minimum, max, decay rate\r\n",
        "    sizes = compounding(1.0, 4.0, 1.001)\r\n",
        "\r\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\r\n",
        "\r\n",
        "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\r\n",
        "\r\n",
        "        for itn in range(iterations):\r\n",
        "\r\n",
        "            file = open(outlog_txt, 'a')  # For logging losses of iterations\r\n",
        "\r\n",
        "            start = time.time()  # Iteration Time\r\n",
        "\r\n",
        "            if itn % 100 == 0:\r\n",
        "                print(\"Itn  : \" + str(itn), time.time() - start_training_time)\r\n",
        "                print('Testing')\r\n",
        "\r\n",
        "                results = evaluate(nlp, TEST_DATA)\r\n",
        "                file1 = open(outlog_file, 'a')\r\n",
        "                file1.write(str(itn) + ',' + str(results['ents_p']) + ',' + str(results['ents_r']) + ',' + str(\r\n",
        "                    results['ents_f']) + ',' + str(results[\"ents_per_type\"]) + \"\\n\")\r\n",
        "                file1.close()\r\n",
        "\r\n",
        "                results = evaluate(nlp, TRAIN_DATA)\r\n",
        "                file2 = open(train_file, 'a')\r\n",
        "                file2.write(str(itn) + ',' + str(results['ents_p']) + ',' + str(results['ents_r']) + ',' + str(\r\n",
        "                    results['ents_f']) + ',' + str(results[\"ents_per_type\"]) + \"\\n\")\r\n",
        "                file2.close()\r\n",
        "\r\n",
        "                modelfile = \"training_model\" + str(itn)\r\n",
        "                nlp.to_disk(modelfile)\r\n",
        "\r\n",
        "            # Reducing Learning rate after certain operations \r\n",
        "            if itn == 150:\r\n",
        "                optimizer.learn_rate = 0.0001\r\n",
        "\r\n",
        "            print(\"Statring iteration \" + str(itn))\r\n",
        "            random.shuffle(TRAIN_DATA)\r\n",
        "            losses = {}\r\n",
        "\r\n",
        "            # use either batches or entire set at once\r\n",
        "\r\n",
        "            ##### For training in Batches\r\n",
        "            batches = minibatch(TRAIN_DATA, size=sizes)\r\n",
        "            for batch in batches:\r\n",
        "                texts, annotations = zip(*batch)\r\n",
        "                nlp.update(texts, annotations, sgd=optimizer, drop=next(dropout), losses=losses)\r\n",
        "\r\n",
        "            ###########################################\r\n",
        "\r\n",
        "            ##### For training in as a single iteration\r\n",
        "\r\n",
        "            # for text, annotations in TRAIN_DATA:\r\n",
        "            #     nlp.update(\r\n",
        "            #         [text],  # batch of texts\r\n",
        "            #         [annotations],  # batch of annotations\r\n",
        "            #         drop=0.2,  # dropout - make it harder to memorise data\r\n",
        "            #         # drop=next(dropout),  Incase you are using decaying drop\r\n",
        "            #         sgd=optimizer,  # callable to update weights\r\n",
        "            #         losses=losses)\r\n",
        "\r\n",
        "            print(\"Losses\", losses)\r\n",
        "            file.write(str(itn) + \",\" + str(losses['ner']) + \"\\n\")\r\n",
        "            print(\"time for iteration:\", time.time() - start)\r\n",
        "            file.close()\r\n",
        "\r\n",
        "    return nlp\r\n",
        "\r\n",
        "\r\n",
        "def evaluate(ner_model, test_data):\r\n",
        "    scorer = Scorer()\r\n",
        "    for input_, annot in test_data:\r\n",
        "        doc_gold_text = ner_model.make_doc(input_)\r\n",
        "        # gold = Example.from_dict(doc_gold_text, {\"entities\": annot['entities']})\r\n",
        "        gold = GoldParse(doc_gold_text, entities=annot['entities'])\r\n",
        "        pred_value = ner_model(input_)\r\n",
        "        scorer.score(pred_value, gold)\r\n",
        "    return scorer.scores\r\n",
        "\r\n",
        "\r\n",
        "prdnlp = train_spacy(TRAIN_DATA, 200)\r\n",
        "\r\n",
        "# Save our trained Model\r\n",
        "\r\n",
        "# uncomment if you want to put model name through command line\r\n",
        "# modelfile = input(\"Enter your Model Name: \")\r\n",
        "modelfile = \"Final_model\"\r\n",
        "prdnlp.to_disk(modelfile)\r\n",
        "\r\n",
        "# Test your text\r\n",
        "# test_text = input(\"Enter your testing text: \")\r\n",
        "# doc = prdnlp(test_text)\r\n",
        "# for ent in doc.ents:\r\n",
        "#     print(ent.text, ent.start_char, ent.end_char, ent.label_)\r\n",
        "\r\n",
        "# Prints Final -- f1 score, precision and recall\r\n",
        "results = evaluate(prdnlp, TEST_DATA)\r\n",
        "import json\r\n",
        "\r\n",
        "print(json.dumps(results, indent=4))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created blank 'indo' model\n",
            "Itn  : 0 16.783547401428223\n",
            "Testing\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}